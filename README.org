#+TITLE: Modelling Semantic Relations with Distributional Semantics and Deep Learning: Question Answering, Entailment Recognition and Paraphrase Detection

* Prerequisites
   1. Download source code
   2. You will need *Python version 3.4.3 or higher*
   3. Install required packages:
      #+BEGIN_SRC sh :exports code
        pip install -r requirements.txt
      #+END_SRC
   4. If you want to run experiments that require word embeddings you need to download and extract distributional models:
      #+BEGIN_SRC sh :exports code
        wget http://lxcenter.di.fc.ul.pt/datasets/msrdsdl.tar.gz
      #+END_SRC

      #+BEGIN_SRC sh :exports code
        tar -xvf vlad.tar.gz
      #+END_SRC

* About the program
After installing all the prerequisites you will have the following directory structure: 
#+BEGIN_SRC 
├── cnn.py                    # Main application		             
├── data               	      # Datasets: 			     
│   ├── askubuntu      	      # - QA: AskUbuntu		     
│   │   ├── test.tsv	                                            
│   │   ├── train.tsv	                                            
│   │   └── val.tsv	                                            
│   ├── meta           	      # - QA: Meta StackExchange	     
│   │   ├── test.tsv	                                            
│   │   ├── train.tsv	                                            
│   │   └── val.tsv	                                            
│   ├── pt             	      # - Portuguese Entailment Recognition 
│   │   ├── test.tsv	                                            
│   │   └── train.tsv	                                            
│   └── ru             	      # - Russian Paraphrase Detection      
│       ├── test.tsv	                                            
│       └── train.tsv	                                            
├── __init.py__		                                            
├── models             	      # - Distributional semantic models    
│   ├── askubuntu.w2v	                                            
│   ├── meta.w2v	                                            
│   ├── pt.w2v		                                            
│   └── ruscorpora.model.w2v				     
├── preprocess.py      	      # Preprocessing functions	     
├── README.org
└── requirements.txt
#+END_SRC

You can run the application =./cnn.py= with the =--help= argument to see available parameters.

To change hyperparameters you can modify the method =SentenceSimilarity. set_hyperparameters= by adding new modes. 

* Replication of my work
** Question Answering
*** Replication of the work by Bogdanova et al. (2015)
    For AskUbuntu dataset:
    #+BEGIN_SRC sh :exports code
      ./cnn.py replication --train data/askubuntu/clue/train.tsv \
               --val data/askubuntu/clue/val.tsv \
               --test data/askubuntu/clue/test.tsv \
               --w2v models/askubuntu.w2v
    #+END_SRC
    For META Stackexchange dataset:
    #+BEGIN_SRC sh :exports code
      ./cnn.py replication --train data/meta/clue/train.tsv \
               --val data/meta/clue/val.tsv \
               --test data/meta/clue/test.tsv \
               --w2v models/meta.w2v
    #+END_SRC

*** Impact of text preprocessing
    For AskUbuntu dataset:
    #+BEGIN_SRC sh :exports code
      ./cnn.py pp_impact --train data/meta/noclue/train.tsv \
               --val data/meta/val.tsv \
               --test data/askubuntu/test.tsv \
               --w2v models/askubuntu.w2v
    #+END_SRC
    For META Stackexchange dataset:    
    #+BEGIN_SRC sh :exports code
      ./cnn.py pp_impact --train data/meta/noclue/train.tsv \
               --val data/meta/val.tsv \
               --test data/meta/test.tsv \
               --w2v models/meta.w2v
    #+END_SRC

*** Impact of word embeddings
    #+BEGIN_SRC sh :exports code
      ./cnn.py we_impact --train data/meta/noclue/train.tsv \
               --val data/meta/val.tsv \
               --test data/askubuntu/test.tsv
    #+END_SRC

** Portuguese Entailment Recognition
*** Run 1
    #+BEGIN_SRC sh :exports code
      ./cnn_padded.py pt_1 --train data/pt/train.tsv \
               --test data/pt/test.tsv \
               --w2v models/pt.w2v
    #+END_SRC

*** Run 2
    #+BEGIN_SRC sh :exports code
      ./cnn_padded.py --train data/pt/train.tsv --test data/pt/test.tsv
    #+END_SRC

** Russian Paraphrase Detection
*** Non-standard run 
    #+BEGIN_SRC sh :exports code
      ./cnn.py ru_ns --train data/ru/train.tsv \
               --val data/ru/test.tsv \
               --w2v models/ruscorpora.model.w2v
    #+END_SRC

*** Standard run
**** Word embeddings
    #+BEGIN_SRC sh :exports code
      ./cnn.py ru_word --train data/ru/train.tsv --val data/ru/test.tsv
    #+END_SRC

**** Character embeddings
    #+BEGIN_SRC sh :exports code
      ./cnn.py ru_char --train data/ru/train.tsv --val data/ru/test.tsv
    #+END_SRC
